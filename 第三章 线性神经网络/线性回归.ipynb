{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性神经网络\n",
    "\n",
    "在了解深度神经网络细节之前，我们需要补充一些神经网络训练的基础。本章内容覆盖整个训练过程，包括定义简单的神经网络结构，处理数据，设定一个损失函数，以及训练模型。为了让读者更好的掌握，我们会从一些简单的概念开始。幸运地是，一些经典数据学习工具，例如线性, *softmax* 回归函数可以被归类进线性神经网路中。从这些经典的算法开始，我们将会向读者介绍这些基础，它们会为本书接下来的部分打下基础。\n",
    "\n",
    "## 3.1 线性回归\n",
    "\n",
    "回归指的是使用一系列建模方法塑造数据点x和与之对应的真值y之间的关系。在自然和社会科学中，回归的目的大多时候是描绘输入和输出之间的关系。机器学习，则是更关心*预测*。\n",
    "\n",
    "当我们想要预测一个算数值时，回归问题无处不在。在数不胜数的常规例子中包括预测价格（房价，股市之类），预测停留时间（病人在医院的时间），需求预测（零售）。但不是每一个预测问题都是经典的回归问题。在子章节中，我们将会介绍分类问题。这类问题的目标就是在一系列类别中预测其成员。\n",
    "\n",
    "### 3.1.1 线性回归中基础元素\n",
    "\n",
    "在标准回归中，线性回归可能是最简单也是最流行的工具。追溯到19世纪，线性回归产生于一些简单假设。首先，我们假设特征x与目标y之间的关系是线性的，例如y可以被视为是一种输入x的加权和，只是在观察中自带或外加了一些噪声。其次，我们假设任何噪声都可以是良序的（比如服从高斯分布）。为了更好地阐述这种方法，让我们从一个例子开始。假设我们希望预测房价（美元为货币单位），考虑因素包括面积（平方米）和房龄（以年为单位）。\n",
    "\n",
    "为了更好地得到一个预测房价的模型，我们需要一个销售记录数据集，它包含出售价、面积、房龄。在机器学习的术语中，这个数据集被称为训练数据或是训练集。它的每一行（数据对应一次销售记录）都是一个示例（数据点，样本）。我们试图预测的（此处是房价）被称为*label标签*（或是目标）。其中的变量（这里是面积和房龄）是这次预测中的基础，被称之为*features特征*或是*covariates相关变量*。\n",
    "\n",
    "通常，我们会用n 去标记我们数据集中的示例数量，我们用i索引每一个数据示例，用\n",
    "$$\n",
    "x^{(i)}=\\left[x_{1}^{(i)}, x_{2}^{(i)}\\right]\n",
    "$$\n",
    "表示，对应的标签写为\n",
    "$$\n",
    "y^{(i)}\n",
    "$$\n",
    "\n",
    "#### 3.1.1.1 线性模型\n",
    "\n",
    "在线性假设中，目标（房价）可以被表达为一个特征（面积和房龄）的加权和：\n",
    "$$\n",
    "price =w_{\\text {area }} \\cdot area +w_{\\text {age }} \\cdot age +b\n",
    "$$\n",
    "这里的W_area 和 Wage被成为*weight权重*，b则是*bias偏量*。权重决定每一个特征在我们的预测中的影响，偏量则是说当所有特征取0时，预测的价格值是多少。即使房子不可能面积为0，或者房龄为0，我们仍然需要截距，不然我们的线性模型表达性将会受到局限。\n",
    "\n",
    "在给定的数据集中，我们的目标是平均地选择权重w和偏量b，使模型对应的预测更加符合数据集中的真实价格。\n",
    "\n",
    "原则上，通常我们只专注与数据集中的一部分特征，比如明确表达模型的具体格式。在机器学习中，我们一般会与高维度的数据集打交道，那么使用线性算数表达更为方便。当我们的输入包含d种特征，我们可以这样表达预测y：\n",
    "$$\n",
    "\\hat{y}=w_{1} \\cdot x_{1}+\\ldots+w_{d} \\cdot x_{d}+b\n",
    "$$\n",
    "集合所有特征将它们传入一个向量x，所有权重传入一个向量w，这样可以用点乘简洁地表示我们的模型：\n",
    "$$\n",
    "\\hat{y}=\\mathbf{w}^{\\top} \\mathbf{x}+b\n",
    "$$\n",
    "此时，向量x对应单个数据点。我们一般会找到当定的矩阵X，更简单地描述整个数据集。这里，X中的每一行表示一个示例，每一列标记一种特征。\n",
    "\n",
    "对应数据点的集合X，预测值y通过矩阵向量乘法写成：\n",
    "$$\n",
    "\\hat{\\mathbf{y}}=\\mathbf{X} \\mathbf{w}+b\n",
    "$$\n",
    "在给定数据集X和已知对应目标y的情况下，线性回归的目的是当输入一个新的数据点xi时，找到权重向量w和偏量级数b。从相同的分布中标记示例作为训练集，将会以最小错误率预测目标yi。\n",
    "\n",
    "即使我们相信已知x预测y的最佳模型是线性的，我们也不可能在现实世界中找到一个具体的yi使所有点(x, y)完全符合等式w^Tx+b。比如无论什么乐器，我们可观测到的特征和标签都会被一些测量错误影响。因此，即使当我们对潜在关系是线性的结论很自信，我们也会吸收一个噪声级数被算作错误。\n",
    "\n",
    "在我们找寻最佳参数w和b之前，我们需要做另外两件事：1，衡量模型好坏的标准；2，更新模型以提高质量的步骤。\n",
    "\n",
    "#### 3.1.1.2 损失函数\n",
    "\n",
    "在我们开始思考如何适配模型之前，我们应该决定适配的衡量标准。损失函数的作用是量化真实值和目标预测值之间的距离。损失值一般是一个数值较小非负数。一次完美的预测，损失即为0。在回归问题中，平方和误差是被广泛使用的损失函数。对于每一个示例i，将会做一次对应的预测y^i帽子，与之对应的真值标签为y^i，那么平方差之和为：\n",
    "$$\n",
    "l^{(i)}(\\mathbf{w}, b)=\\frac{1}{2}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2}\n",
    "$$\n",
    "有没有常数1/2没有什么真的区别只是在计算中方便抵消损失的导数。因为我们手中已经有了训练集，因此在我们的掌控中，经验误差只是一个模型参数函数。更具体地，考虑到接下来我们将在fig_fit_linreg中展示一个一维回归问题。\n",
    "\n",
    "请注意，预测值y和观察真值y之间的差值，因为损失是平方的结果，造成损失产生更大差值。为了衡量整个数据集，我们会求取训练集的平均损失（或者相等、求和）。\n",
    "$$\n",
    "L(\\mathbf{w}, b)=\\frac{1}{n} \\sum_{i=1}^{n} l^{(i)}(\\mathbf{w}, b)=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b-y^{(i)}\\right)^{2}\n",
    "$$\n",
    "当我们训练模型时，我们希望找到参数（w, b）使得在所有训练示例中得到最小损失。\n",
    "$$\n",
    "\\mathbf{w}^{*}, b^{*}=\\underset{\\mathbf{w}, b}{\\operatorname{argmin}} L(\\mathbf{w}, b)\n",
    "$$\n",
    "\n",
    "#### 3.1.1.3 闭合解/解析解\n",
    "\n",
    "当我们遇到非常规的简单优化问题时，需要线性回归的帮助。不像本书中大多数其他模型，线性回归可以通过一些简单的公式解析分解，从而得出一个全局最优解。一开始，我们可以通过附加一列到含有所有1s的特定矩阵，将偏量b纳入参数w。然后我们的预测问题简化成了使y-xw最小化问题。欣慰这种表达是一种二次齐式的，凸形的。只要这个问题不能退化（我们的特征是线性非相关），那么就是严格凸性。\n",
    "\n",
    "因此，这里在损失平面上存在一个全局最小值的临界点。对w取损失的导数并将等式左边等于0，可以得到解析解：\n",
    "$$\n",
    "\\mathbf{w}^{*}=\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y}\n",
    "$$\n",
    "当简单问题像线性回归一样得到解析解，不要想当然认为以后都将一帆风顺。尽管解析解允许看起来很棒的数学分析，但是对于解析解的要求是有严苛限制的，以至于在深度学习中无法使用解析解。\n",
    "\n",
    "#### 3.1.1.4 梯度下降\n",
    "\n",
    "即使在无法使用解析解求解模型的情况下，甚至在损失平面是高维且非凸的情况下，事实证明我们仍然可以在实践中有效地训练模型。并且，对于许多任务来说，这些难以优化的模型表现得要好得多，以至于弄清楚如何训练它们比这些问题本身要有价值得多。\n",
    "\n",
    "关键点在于优化任意的深度学习模型，\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.9+7-LTS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
