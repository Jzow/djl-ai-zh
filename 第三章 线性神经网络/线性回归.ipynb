{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性神经网络\n",
    "\n",
    "在了解深度神经网络细节之前，我们需要补充一些神经网络训练的基础。本章内容覆盖整个训练过程，包括定义简单的神经网络结构，处理数据，设定一个损失函数，以及训练模型。为了让读者更好的掌握，我们会从一些简单的概念开始。幸运地是，一些经典数据学习工具，例如线性, *softmax* 回归函数可以被归类进线性神经网路中。从这些经典的算法开始，我们将会向读者介绍这些基础，它们会为本书接下来的部分打下基础。\n",
    "\n",
    "## 3.1 线性回归\n",
    "\n",
    "回归指的是使用一系列建模方法塑造数据点x和与之对应的真值y之间的关系。在自然和社会科学中，回归的目的大多时候是描绘输入和输出之间的关系。机器学习，则是更关心*预测*。\n",
    "\n",
    "当我们想要预测一个算数值时，回归问题无处不在。在数不胜数的常规例子中包括预测价格（房价，股市之类），预测停留时间（病人在医院的时间），需求预测（零售）。但不是每一个预测问题都是经典的回归问题。在子章节中，我们将会介绍分类问题。这类问题的目标就是在一系列类别中预测其成员。\n",
    "\n",
    "### 3.1.1 线性回归中基础元素\n",
    "\n",
    "在标准回归中，线性回归可能是最简单也是最流行的工具。追溯到19世纪，线性回归产生于一些简单假设。首先，我们假设特征x与目标y之间的关系是线性的，例如y可以被视为是一种输入x的加权和，只是在观察中自带或外加了一些噪声。其次，我们假设任何噪声都可以是良序的（比如服从高斯分布）。为了更好地阐述这种方法，让我们从一个例子开始。假设我们希望预测房价（美元为货币单位），考虑因素包括面积（平方米）和房龄（以年为单位）。\n",
    "\n",
    "为了更好地得到一个预测房价的模型，我们需要一个销售记录数据集，它包含出售价、面积、房龄。在机器学习的术语中，这个数据集被称为训练数据或是训练集。它的每一行（数据对应一次销售记录）都是一个示例（数据点，样本）。我们试图预测的（此处是房价）被称为*label标签*（或是目标）。其中的变量（这里是面积和房龄）是这次预测中的基础，被称之为*features特征*或是*covariates相关变量*。\n",
    "\n",
    "通常，我们会用n 去标记我们数据集中的示例数量，我们用i索引每一个数据示例，用\n",
    "$$\n",
    "x^{(i)}=\\left[x_{1}^{(i)}, x_{2}^{(i)}\\right]\n",
    "$$\n",
    "表示，对应的标签写为\n",
    "$$\n",
    "y^{(i)}\n",
    "$$\n",
    "\n",
    "#### 3.1.1.1 线性模型\n",
    "\n",
    "在线性假设中，目标（房价）可以被表达为一个特征（面积和房龄）的加权和：\n",
    "$$\n",
    "price =w_{\\text {area }} \\cdot area +w_{\\text {age }} \\cdot age +b\n",
    "$$\n",
    "这里的W_area 和 Wage被成为*weight权重*，b则是*bias偏量*。权重决定每一个特征在我们的预测中的影响，偏量则是说当所有特征取0时，预测的价格值是多少。即使房子不可能面积为0，或者房龄为0，我们仍然需要截距，不然我们的线性模型表达性将会受到局限。\n",
    "\n",
    "在给定的数据集中，我们的目标是平均地选择权重w和偏量b，使模型对应的预测更加符合数据集中的真实价格。\n",
    "\n",
    "原则上，通常我们只专注与数据集中的一部分特征，比如明确表达模型的具体格式。在机器学习中，我们一般会与高维度的数据集打交道，那么使用线性算数表达更为方便。当我们的输入包含d种特征，我们可以这样表达预测y：\n",
    "$$\n",
    "\\hat{y}=w_{1} \\cdot x_{1}+\\ldots+w_{d} \\cdot x_{d}+b\n",
    "$$\n",
    "集合所有特征将它们传入一个向量x，所有权重传入一个向量w，这样可以用点乘简洁地表示我们的模型：\n",
    "$$\n",
    "\\hat{y}=\\mathbf{w}^{\\top} \\mathbf{x}+b\n",
    "$$\n",
    "此时，向量x对应单个数据点。我们一般会找到当定的矩阵X，更简单地描述整个数据集。这里，X中的每一行表示一个示例，每一列标记一种特征。\n",
    "\n",
    "对应数据点的集合X，预测值y通过矩阵向量乘法写成：\n",
    "$$\n",
    "\\hat{\\mathbf{y}}=\\mathbf{X} \\mathbf{w}+b\n",
    "$$\n",
    "在给定数据集X和已知对应目标y的情况下，线性回归的目的是当输入一个新的数据点xi时，找到权重向量w和偏量级数b。从相同的分布中标记示例作为训练集，将会以最小错误率预测目标yi。\n",
    "\n",
    "即使我们相信已知x预测y的最佳模型是线性的，我们也不可能在现实世界中找到一个具体的yi使所有点(x, y)完全符合等式w^Tx+b。比如无论什么乐器，我们可观测到的特征和标签都会被一些测量错误影响。因此，即使当我们对潜在关系是线性的结论很自信，我们也会吸收一个噪声级数被算作错误。\n",
    "\n",
    "在我们找寻最佳参数w和b之前，我们需要做另外两件事：1，衡量模型好坏的标准；2，更新模型以提高质量的步骤。\n",
    "\n",
    "#### 3.1.1.2 损失函数\n",
    "\n",
    "在我们开始思考如何适配模型之前，我们应该决定适配的衡量标准。损失函数的作用是量化真实值和目标预测值之间的距离。损失值一般是一个数值较小非负数。一次完美的预测，损失即为0。在回归问题中，平方和误差是被广泛使用的损失函数。对于每一个示例i，将会做一次对应的预测y^i帽子，与之对应的真值标签为y^i，那么平方差之和为：\n",
    "$$\n",
    "l^{(i)}(\\mathbf{w}, b)=\\frac{1}{2}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2}\n",
    "$$\n",
    "有没有常数1/2没有什么真的区别只是在计算中方便抵消损失的导数。因为我们手中已经有了训练集，因此在我们的掌控中，经验误差只是一个模型参数函数。更具体地，考虑到接下来我们将在fig_fit_linreg中展示一个一维回归问题。\n",
    "\n",
    "请注意，预测值y和观察真值y之间的差值，因为损失是平方的结果，造成损失产生更大差值。为了衡量整个数据集，我们会求取训练集的平均损失（或者相等、求和）。\n",
    "$$\n",
    "L(\\mathbf{w}, b)=\\frac{1}{n} \\sum_{i=1}^{n} l^{(i)}(\\mathbf{w}, b)=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b-y^{(i)}\\right)^{2}\n",
    "$$\n",
    "当我们训练模型时，我们希望找到参数（w, b）使得在所有训练示例中得到最小损失。\n",
    "$$\n",
    "\\mathbf{w}^{*}, b^{*}=\\underset{\\mathbf{w}, b}{\\operatorname{argmin}} L(\\mathbf{w}, b)\n",
    "$$\n",
    "\n",
    "#### 3.1.1.3 闭合解/解析解\n",
    "\n",
    "当我们遇到非常规的简单优化问题时，需要线性回归的帮助。不像本书中大多数其他模型，线性回归可以通过一些简单的公式解析分解，从而得出一个全局最优解。一开始，我们可以通过附加一列到含有所有1s的特定矩阵，将偏量b纳入参数w。然后我们的预测问题简化成了使y-xw最小化问题。欣慰这种表达是一种二次齐式的，凸形的。只要这个问题不能退化（我们的特征是线性非相关），那么就是严格凸性。\n",
    "\n",
    "因此，这里在损失平面上存在一个全局最小值的临界点。对w取损失的导数并将等式左边等于0，可以得到解析解：\n",
    "$$\n",
    "\\mathbf{w}^{*}=\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y}\n",
    "$$\n",
    "当简单问题像线性回归一样得到解析解，不要想当然认为以后都将一帆风顺。尽管解析解允许看起来很棒的数学分析，但是对于解析解的要求是有严苛限制的，以至于在深度学习中无法使用解析解。\n",
    "\n",
    "#### 3.1.1.4 梯度下降\n",
    "\n",
    "即使在无法使用解析解求解模型的情况下，甚至在损失平面是高维且非凸的情况下，事实证明我们仍然可以在实践中有效地训练模型。并且，对于许多任务来说，这些难以优化的模型表现得要好得多，以至于弄清楚如何训练它们比这些问题本身要有价值得多。\n",
    "\n",
    "关键点在于优化任意的深度学习模型，它是贯穿这本书的主题，包括通过在损失函数逐渐减小小的方向上，更新参数从而迭代式地减少错误率。这种算法叫做*梯度下降*。在凸性损失平面上，它最终会收敛于全局最小值。就算是非凸性平面，理想情况下，它最终会指向局部最小值。\n",
    "\n",
    "梯度下降最基本的应用包括计算真实损失的偏差，它是数据集中每一个样本平均值损失的平均值。在实际中，这种计算需要消耗大量的时间。我们必须在更新单例样本之前遍历整个数据集。因此，通常当我们需要计算更新参数时，会安排一个示例的随机分组取样。这种变形，叫做随机梯度下降。\n",
    "\n",
    "在每一次迭代中，我们首先随机取样一个小批量$\\mathbf{B}$ 包含固定数量的训练示例。然后，在这个随机小批量中根据模型参数计算平均损失的梯度。最后，梯度乘以预先设定的步长$\\eta>0$并从现有参数值中减去剩余项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以用下面的数学式表示更新（偏微分符号$\\partial$）：\n",
    "\n",
    "$(\\mathbf{w}, b) \\leftarrow(\\mathbf{w}, b)-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w}, b)} l^{(i)}(\\mathbf{w}, b)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下，这种算法的步骤如下：(i)初始化模型参数值，通常这一步骤是随机的。（ii）多次迭代地从数据集中随机分组取样，在负梯度的方向上更新参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二次的损失和线性方程，我们可以用如下的式子表示清楚：需要注意的是${w}$和${x}$是向量。在这里，优雅简洁的向量比用参数级数形式$w_{1}, w_{2}, \\ldots, w_{d}$使数学更具有易读性。\n",
    "\n",
    "$\\mathbf{w} \\leftarrow \\mathbf{w}-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) \\quad=\\mathbf{w}-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b-y^{(i)}\\right)$\n",
    "\n",
    "$b \\leftarrow b-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{b} l^{(i)}(\\mathbf{w}, b) \\quad=b-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b-y^{(i)}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的等式中，$\\mathbf{B}$表示每个小批量中一定数量的示例，$\\eta$表示学习率。需要强调的是，分组的大小和学习率是需要手动调整的，它们的数值通常不能通过训练模型从而学习得到。这些可调参数在每次更新是不变的，因此被称为*超参数Hyper Parameters*。超参数调整是一个选择的过程，通常需要我们基于训练循环的结果来调整全局参数，而这些结果是由数据集中的验证集来评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在经过预设次数的迭代训练（或者遇到需要停止训练的准测），我们记录一下预估模型参数，作$\\hat{\\mathbf{w}}, \\hat{b}$（通常，‘尖角’表示预估值）。请注意，即使我们的函数是真线性且无噪声的，这些参数也不会精确到损失最小，因为尽管算法向局部最小缓慢收敛，但它无法在有限的步长中精确地被实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归恰好是解决凸性学习问题，因此只有一个（全局）最小值。然而，对于很多负责的模型来说，比如深网络，损失平面包含很多最小值，让问题变得棘手起来。不过好在，虽然原因至今我们不能完全理解，但是深度学习的实践者们已经很少为了在训练集中，找合适参数使损失最小的这件事上苦苦挣扎了。更艰巨的任务是找到一些神奇的参数，这些参数可以实现在数据集中从未见过的低损失，这种挑战叫做泛化。我们将会在后面的内容重新回到这个话题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.5 使用训练好的模型做预测\n",
    "\n",
    "已知训练好的线性回归模型$\\hat{\\mathbf{w}}^{\\top} \\mathbf{x}+\\hat{b}$，现在我们可以通过面积$x_1$和房龄$x_2$预估一套新房子的价格（不包含与训练集中）。通过已知的特征来进行预估目标，通常称为*预测Prediction*和*推理Inference*。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在这里还是称这个步骤为预测，因为尽管推理逐渐融合到深度学习的术语中，但将这一步称之为推理仍然是不恰当的。在统计学中，推理通常表示基于数据集的参数估计。当深度学习使用者与统计学家交谈时，这种术语的误用会造成混乱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.6 速度的矢量化\n",
    "\n",
    "当我们训练模型时，一般我们希望同时处理所有小批量示例。高效地完成这一步需要我们将计算向量化并利用快速的线性代数库，而不是在Java中写费时费力的for循环。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了阐述这件事为什么如此重要，我们需要两种加入向量的方法。一开始，我们实例化两个10000维的向量。一种方法，我们将使用Java的for循环遍历向量。另一种方法，我们基于DJL进行一次单个调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mavenRepo snapshots https://oss.sonatype.org/content/repositories/snapshots/\n",
    "\n",
    "%maven ai.djl:api:0.9.0-SNAPSHOT\n",
    "%maven org.slf4j:slf4j-api:1.7.26\n",
    "%maven org.slf4j:slf4j-simple:1.7.26\n",
    "\n",
    "// See https://github.com/awslabs/djl/blob/master/mxnet/mxnet-engine/README.md\n",
    "// for more MXNet library selection options\n",
    "%maven ai.djl.mxnet:mxnet-engine:0.9.0-SNAPSHOT\n",
    "%maven ai.djl.mxnet:mxnet-native-auto:1.7.0-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "<dependency>\n",
    "    <groupId>tech.tablesaw</groupId>\n",
    "    <artifactId>tablesaw-jsplot</artifactId>\n",
    "    <version>0.38.1</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要一些实用程序。我们可以通过%load macro下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../utils/plot-utils.ipynb\n",
    "%load ../utils/StopWatch.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai.djl.ndarray.*;\n",
    "import ai.djl.ndarray.types.*;\n",
    "import ai.djl.ndarray.index.*;\n",
    "\n",
    "import java.util.*;\n",
    "import java.util.stream.*;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int n = 10000;\n",
    "NDManager manager = NDManager.newBaseManager();\n",
    "NDArray a = manager.ones(new Shape(n));\n",
    "NDArray b = manager.ones(new Shape(n));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以对工作负载进行基准测试。首先，我们使用for循环，将它们一次添加一个坐标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDArray c = manager.zeros(new Shape(n));\n",
    "StopWatch stopWatch = new StopWatch();\n",
    "for (int i = 0; i < n; i++) {\n",
    "    c.set(new NDIndex(i), a.getFloat(i) + b.getFloat(i));\n",
    "}\n",
    "String.format(\"%.5f sec\", stopWatch.stop());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者，我们用DJL来计算元素和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWatch.start();\n",
    "NDArray d = a.add(b);\n",
    "String.format(\"%.5f sec\", stopWatch.stop());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你大概注意到两种方法中，后者比前者快很多。向量化代码通常会产生数量级的加速。并且，我们将更多的数学运算集成到了库中从而不需要自己写很多的计算，这样减少了很多错误的几率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.9+7-LTS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
